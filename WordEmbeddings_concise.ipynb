{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmbeddings_concise.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VP5tD4F2LNQ5",
        "ysZXG1V1Ox4M",
        "u8wvnSX4Pcq8",
        "IXlbAsBCYggS",
        "EEQ59UjRTcpb",
        "qmv3vD7quDZ0",
        "6W1jrEUxcadc",
        "xZgiIr7Mc7lT",
        "MW5c_63-a497"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP5tD4F2LNQ5",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99tVsvbbfShn",
        "colab_type": "text"
      },
      "source": [
        "#### Packages setup and imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko7BoMCcQ1M_",
        "colab_type": "text"
      },
      "source": [
        "As of 6/20/2020 some TensorFlow nightly build features offer improved training speed over 2.2.0. This cell is optional if such performance gains aren't necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVJm8l96dQVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-nightly                     # ONLY IF YOU DO NEED IT\n",
        "!pip uninstall -q tensorboard tb-nightly    # THIS FIXES ISSUES WITH DUPLICATE TENSORBOARDS\n",
        "!pip install -q tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Viz5Q3NpLQ-k",
        "colab_type": "text"
      },
      "source": [
        "Download and install WordEmbeddings package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfISgXZqLDcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "d4a4ffce-3b9a-4a25-95f0-e358e93b88c1"
      },
      "source": [
        "!git clone -b gcp https://github.com/Flomastruk/wordembeddings.git\n",
        "%cd wordembeddings/\n",
        "!pip install .\n",
        "%cd /content/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'wordembeddings'...\n",
            "remote: Enumerating objects: 193, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 193 (delta 108), reused 136 (delta 56), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (193/193), 1.69 MiB | 1.83 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n",
            "/content/wordembeddings\n",
            "Processing /content/wordembeddings\n",
            "Building wheels for collected packages: wordembeddings\n",
            "  Building wheel for wordembeddings (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordembeddings: filename=wordembeddings-0.0.0-cp36-none-any.whl size=22175 sha256=0661953bc6a2245e90d9605df0674608c26c149af4b84d8c0ba8caf1058a6e31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fwag7ond/wheels/7d/69/30/2bc46802895f7cb924804b5ba49719e836ebce59e6a9b27714\n",
            "Successfully built wordembeddings\n",
            "Installing collected packages: wordembeddings\n",
            "Successfully installed wordembeddings-0.0.0\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQIOWRR4StsY",
        "colab_type": "text"
      },
      "source": [
        "The cell below is necessary if job directory is intended to be inside Google Drive. Fully optional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI2nrGyTRRB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d54b716d-ea9e-46ff-ed3e-53c2e37feb0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysZXG1V1Ox4M",
        "colab_type": "text"
      },
      "source": [
        "## Training the model from command line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCFsAU9qVOsT",
        "colab_type": "text"
      },
      "source": [
        "Create a job directory and (optionally) copy tests inside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgp7bA2UQCI1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6960a119-3b65-4167-ce51-31998bac191f"
      },
      "source": [
        "%env JOB_DIR=/content/wordembeddings_jobs\n",
        "!mkdir $JOB_DIR\n",
        "!cp -r /content/wordembeddings/tests $JOB_DIR/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: JOB_DIR=/content/wordembeddings_jobs\n",
            "mkdir: cannot create directory ‘/content/wordembeddings_jobs’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsZQ5NyyPK2y",
        "colab_type": "text"
      },
      "source": [
        "The command below launches training process inside the chosen job directory and saves model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjRZxpgeOxfP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0df73129-8924-4259-8ed5-d1829228c176"
      },
      "source": [
        "!python -m newmodel.task --job-dir=/content/wordembeddings_jobs  --mode=glove --log-dir=glove_demo_model --save-dir=glove_demo_model --corpus-name=enwik8 --epochs=1 --min-occurrence=10 --max-vocabulary-size=50000 --skip-window=10"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-23 04:11:29.755999: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-23 04:11:31.755409: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-06-23 04:11:31.774222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-23 04:11:31.774977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-06-23 04:11:31.775027: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-23 04:11:31.777242: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-23 04:11:31.779326: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-23 04:11:31.779678: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-23 04:11:31.783777: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-23 04:11:31.785507: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-23 04:11:31.791557: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-23 04:11:31.791746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-23 04:11:31.792543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-23 04:11:31.793295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-06-23 04:11:31.793856: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-06-23 04:11:31.801945: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-06-23 04:11:31.802189: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b74bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-06-23 04:11:31.802231: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-06-23 04:11:31.858360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-23 04:11:31.859225: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b74a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-06-23 04:11:31.859264: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-06-23 04:11:31.859594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-23 04:11:31.860392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-06-23 04:11:31.860484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-23 04:11:31.860544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-23 04:11:31.860594: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-23 04:11:31.860641: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-23 04:11:31.860685: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-23 04:11:31.860758: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-23 04:11:31.860819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-23 04:11:31.860956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-23 04:11:31.861790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-23 04:11:31.862493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-06-23 04:11:31.862552: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-23 04:11:32.404364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-23 04:11:32.404434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-06-23 04:11:32.404459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-06-23 04:11:32.404750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-23 04:11:32.405593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-23 04:11:32.406437: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-06-23 04:11:32.406514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10620 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Key and value files for stored_enwik8_maxsize_50000_minocc_10_window_10_storedbatch_131072 already exist. Nothing to be done. Consider checking contents.\n",
            "Found following similarity tests:\n",
            "{'WSrel': '/content/wordembeddings_jobs/tests/similarity_tests/wordsim_relatedness_goldstandard.tsv', 'SX': '/content/wordembeddings_jobs/tests/similarity_tests/simlex999.tsv', 'MT': '/content/wordembeddings_jobs/tests/similarity_tests/mturk771.tsv', 'WSsim': '/content/wordembeddings_jobs/tests/similarity_tests/wordsim_similarity_goldstandard.tsv', 'WS': '/content/wordembeddings_jobs/tests/similarity_tests/wordsim353.tsv', 'MEN': '/content/wordembeddings_jobs/tests/similarity_tests/men.tsv', 'RG': '/content/wordembeddings_jobs/tests/similarity_tests/RG-65.tsv', 'RW': '/content/wordembeddings_jobs/tests/similarity_tests/rw_processed.tsv', 'MC': '/content/wordembeddings_jobs/tests/similarity_tests/MC-30.tsv', 'SV': '/content/wordembeddings_jobs/tests/similarity_tests/simverb3500.tsv'}\n",
            "Found following analogy tests:\n",
            "{'QW': '/content/wordembeddings_jobs/tests/analogy_tests/questions-words.txt'}\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "2020-06-23 04:11:48.506157: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "\n",
            "Epoch 00001: saving model to /content/wordembeddings_jobs/saved_models/glove_demo_model/cp-0001.ckpt\n",
            "Saving model configuration to /content/wordembeddings_jobs/saved_models/glove_demo_model\n",
            "1016/1016 [==============================] - 164s 162ms/step - loss: 0.1343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8wvnSX4Pcq8",
        "colab_type": "text"
      },
      "source": [
        "## Training the model using WordEmbeddings package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXlbAsBCYggS",
        "colab_type": "text"
      },
      "source": [
        "#### Importing required classes and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNVjYL6IPrZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from newmodel.util import load_process_data, create_dataset_from_stored_batches, normalized_train_file_name, lr_scheduler_factory\n",
        "from newmodel.model import GloveModel, HypGloveModel, Word2VecModel\n",
        "from newmodel.tests import get_similarity_tests, get_analogy_tests\n",
        "from newmodel.tests import ANALOGY_TEST_GROUPS\n",
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEQ59UjRTcpb",
        "colab_type": "text"
      },
      "source": [
        "#### Main training parameters definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAg0hOS-gHYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# `job_dir` must be a path to a directory where data, models and logs will be stored\n",
        "job_dir = '/content/wordembeddings_jobs' # Can be a path in Google Drive if enabled above '/content/drive/My Drive/word2vec/newmodel' \n",
        "\n",
        "# note: paths below are relative to `job_dir`\n",
        "log_dir = 'word_embedding_model'        # optional, if None model logs are written to `logs/temp`\n",
        "restore_dir = None                      # optional, if specified assumes that the model was saved previously in `restore_dir`\n",
        "save_dir = 'word_embedding_model'       # optional, if None the model is not saved\n",
        "\n",
        "# model settings\n",
        "mode = 'glove'              # Available: 'glove' 'hypglove' word2vec'\n",
        "embedding_size = 200\n",
        "\n",
        "# data processing settings\n",
        "corpus_name = 'enwik8'      # Available: 'enwik8' 'enwik9' 'enwiki_dump'\n",
        "max_vocabulary_size = 50000\n",
        "min_occurrence = 10\n",
        "skip_window = 10            # window size in skipgram model (2-sided)\n",
        "\n",
        "# training settings\n",
        "num_epochs = 2              # number of epochs to train the model (if the model is restored the training continues)\n",
        "learning_rate = 3e-3        # default initial learning rate\n",
        "batch_size = 2**15\n",
        "stored_batch_size = 2**17   # must be divisible by batch_size, affects how the data is stored on the disk\n",
        "neg_samples = 0             # only used in models using negative sampling\n",
        "po = 0.75                   # parameters used in word cooccurrence re-weighing\n",
        "threshold = 100"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM6u78LOrFTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# assign selected settings to a single Namespace \n",
        "from types import SimpleNamespace\n",
        "args = SimpleNamespace()\n",
        "\n",
        "args.job_dir = job_dir\n",
        "args.log_dir = log_dir\n",
        "args.restore_dir = restore_dir\n",
        "args.save_dir = save_dir\n",
        "\n",
        "args.mode = mode\n",
        "args.embedding_size = embedding_size\n",
        "\n",
        "args.corpus_name = corpus_name\n",
        "args.max_vocabulary_size = max_vocabulary_size \n",
        "args.min_occurrence = min_occurrence\n",
        "args.skip_window = skip_window\n",
        "\n",
        "args.num_epochs = num_epochs\n",
        "args.learning_rate = learning_rate\n",
        "args.batch_size = batch_size\n",
        "args.stored_batch_size = stored_batch_size\n",
        "args.neg_samples = neg_samples\n",
        "args.po = po\n",
        "args.threshold = threshold"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBoMoRMkirte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.makedirs(job_dir, exist_ok = True)\n",
        "os.chdir(job_dir) # expects that the directory exists"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmv3vD7quDZ0",
        "colab_type": "text"
      },
      "source": [
        "#### Example 1: Glove Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sByeBLmRxtq4",
        "colab_type": "text"
      },
      "source": [
        "Redefine some example-specific arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw3CGiulxHQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args.save_dir = 'glove_demo_model'      # model is saved in saved_models/glove_demo_model under job_dir\n",
        "args.restore_dir = 'glove_demo_model'   # model is restored from saved_models/ glove_demo_model under job_dir\n",
        "args.log_dir = 'glove_demo_model'       # logs are written to logs/glove_demo_model under job_dir\n",
        "\n",
        "args.mode = 'glove'\n",
        "args.neg_samples = 0 # no negative samples"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VY9R6QuxzSn",
        "colab_type": "text"
      },
      "source": [
        "Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZg87L29uCkw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "278c6dd0-9127-48d8-8952-23f4cecdcaa0"
      },
      "source": [
        "train_file_name = normalized_train_file_name(args)\n",
        "train_file_path = os.path.join(args.job_dir, 'model_data', train_file_name)\n",
        "\n",
        "word2id, id2word, word_counts, id_counts, skips_paths = load_process_data(train_file_name, args, remove_zero = False)\n",
        "vocabulary_size = max(word2id.values()) + 1\n",
        "\n",
        "dataset = create_dataset_from_stored_batches(skips_paths, args.stored_batch_size, batch_size = args.batch_size, sampling_distribution = None, threshold = args.threshold, po = args.po, neg_samples = args.neg_samples)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Key and value files for stored_enwik8_maxsize_50000_minocc_10_window_10_storedbatch_131072 already exist. Nothing to be done. Consider checking contents.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhLImcEOy4ZT",
        "colab_type": "text"
      },
      "source": [
        "Create and compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V18BfrMzu5Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_model = GloveModel(vocabulary_size, args.embedding_size, args.neg_samples, learning_rate=2e-3, word2id = word2id, id2word = id2word)\n",
        "train_model.compile(loss = train_model.loss, optimizer = train_model.optimizer)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6o26dAH6uOt",
        "colab_type": "text"
      },
      "source": [
        "Restore model weights if available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ7HPhnjwD_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f19be2ef-f2b6-42b3-cc4d-dd12f292678f"
      },
      "source": [
        "epochs_trained_ = 0\n",
        "try:\n",
        "    if args.restore_dir:\n",
        "        _, epochs_trained_ = train_model.load_model(os.path.join(args.job_dir, 'saved_models', args.restore_dir))    \n",
        "except FileNotFoundError: # if no file to restore from found\n",
        "    print('Model weights could not be found.')\n",
        "print(f'Epochs trained: {epochs_trained_}')  "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs trained: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vabPrLPr6qaJ",
        "colab_type": "text"
      },
      "source": [
        "Define callbacks for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhqs0INOvKbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2528d1a5-9aad-42de-a38e-3493bed9edf7"
      },
      "source": [
        "similarity_tests_dict = get_similarity_tests(args.job_dir)\n",
        "similarity_callbacks = train_model.get_similarity_tests_callbacks(similarity_tests_dict, ['target', 'context', 'added', 'concat'], ['l2', 'cos'], args.job_dir, args.log_dir)\n",
        "\n",
        "analogy_tests_dict = get_analogy_tests(args.job_dir)\n",
        "analogy_callbacks = train_model.get_analogy_tests_callbacks(analogy_tests_dict, ['target', 'context'], ['l2', 'cos'], args.job_dir, args.log_dir, group_dict= ANALOGY_TEST_GROUPS)\n",
        "\n",
        "save_callbacks = train_model.get_save_callbacks(os.path.join(args.job_dir, 'saved_models' , args.save_dir) if args.save_dir else None, args, period = 1)\n",
        "loss_callback = train_model.get_loss_callback(args.job_dir, args.log_dir)\n",
        "lr_callback = LearningRateScheduler(lr_scheduler_factory(args.learning_rate))\n",
        "\n",
        "callbacks = save_callbacks + [loss_callback] + similarity_callbacks + analogy_callbacks + [lr_callback]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hofTVhUt64MD",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W-AXUAjwjiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "16b2dc3c-f0ca-4947-b15e-cce4b4e624fc"
      },
      "source": [
        "train_model.fit(dataset, epochs = args.num_epochs, callbacks = callbacks, initial_epoch=epochs_trained_)    # 182ms/step"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/2\n",
            "   1016/Unknown - 80s 79ms/step - loss: 0.0536\n",
            "Epoch 00002: saving model to /content/wordembeddings_jobs/saved_models/word_embedding_model/cp-0002.ckpt\n",
            "Saving model configuration to /content/wordembeddings_jobs/saved_models/word_embedding_model\n",
            "1016/1016 [==============================] - 89s 87ms/step - loss: 0.0536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd16fd80668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6W1jrEUxcadc"
      },
      "source": [
        "#### Example 2: HypGlove Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2OrXdlWxcadd"
      },
      "source": [
        "Redefine some example-specific arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3visArI2cadg",
        "colab": {}
      },
      "source": [
        "args.save_dir = 'hypglove_demo_model'      # model is saved in saved_models/glove_demo_model under job_dir\n",
        "args.restore_dir = 'hypglove_demo_model'   # model is restored from saved_models/ glove_demo_model under job_dir\n",
        "args.log_dir = 'hypglove_demo_model'       # logs are written to logs/glove_demo_model under job_dir\n",
        "\n",
        "args.mode = 'hypglove'\n",
        "args.neg_samples = 0 # no negative samples"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E8HTDV_Pcadi"
      },
      "source": [
        "Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TSNYKVxMcadj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a79bc184-010a-491f-aaca-5e30933ecf87"
      },
      "source": [
        "train_file_name = normalized_train_file_name(args)\n",
        "train_file_path = os.path.join(args.job_dir, 'model_data', train_file_name)\n",
        "\n",
        "word2id, id2word, word_counts, id_counts, skips_paths = load_process_data(train_file_name, args, remove_zero = False)\n",
        "vocabulary_size = max(word2id.values()) + 1\n",
        "\n",
        "dataset = create_dataset_from_stored_batches(skips_paths, args.stored_batch_size, batch_size = args.batch_size, sampling_distribution = None, threshold = args.threshold, po = args.po, neg_samples = args.neg_samples)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Key and value files for stored_enwik8_maxsize_50000_minocc_10_window_10_storedbatch_131072 already exist. Nothing to be done. Consider checking contents.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sMxc2W7cadm"
      },
      "source": [
        "Create and compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p82R6OtXcadm",
        "colab": {}
      },
      "source": [
        "train_model = HypGloveModel(vocabulary_size, args.embedding_size, args.neg_samples, learning_rate=2e-3, word2id = word2id, id2word = id2word)\n",
        "train_model.compile(loss = train_model.loss, optimizer = train_model.optimizer)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "940m8CZpcado"
      },
      "source": [
        "Restore model weights if available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C6yWeYqHcadp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a5efe282-3dac-4200-daa5-ea7461683df2"
      },
      "source": [
        "epochs_trained_ = 0\n",
        "try:\n",
        "    if args.restore_dir:\n",
        "        _, epochs_trained_ = train_model.load_model(os.path.join(args.job_dir, 'saved_models', args.restore_dir))    \n",
        "except FileNotFoundError: # if no file to restore from found\n",
        "    print('Model weights could not be found.')\n",
        "print(f'Epochs trained: {epochs_trained_}')  "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model weights could not be found.\n",
            "Epochs trained: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-KiWvIl1cads"
      },
      "source": [
        "Define callbacks for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ms0z0oyvcads",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abe08ae4-d89e-4488-e145-8f8375326ab9"
      },
      "source": [
        "similarity_tests_dict = get_similarity_tests(args.job_dir)\n",
        "similarity_callbacks = train_model.get_similarity_tests_callbacks(similarity_tests_dict, ['target', 'context', 'added', 'concat'], ['l2', 'cos'], args.job_dir, args.log_dir)\n",
        "\n",
        "analogy_tests_dict = get_analogy_tests(args.job_dir)\n",
        "analogy_callbacks = train_model.get_analogy_tests_callbacks(analogy_tests_dict, ['target', 'context'], ['l2', 'cos'], args.job_dir, args.log_dir, group_dict= ANALOGY_TEST_GROUPS)\n",
        "\n",
        "save_callbacks = train_model.get_save_callbacks(os.path.join(args.job_dir, 'saved_models' , args.save_dir) if args.save_dir else None, args, period = 1)\n",
        "loss_callback = train_model.get_loss_callback(args.job_dir, args.log_dir)\n",
        "lr_callback = LearningRateScheduler(lr_scheduler_factory(args.learning_rate))\n",
        "\n",
        "callbacks = save_callbacks + [loss_callback] + similarity_callbacks + analogy_callbacks + [lr_callback]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KJmt2UB3cadu"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OILEAqrHcadu",
        "colab": {}
      },
      "source": [
        "train_model.fit(dataset, epochs = args.num_epochs, callbacks = callbacks, initial_epoch=epochs_trained_)# 182ms/step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZgiIr7Mc7lT",
        "colab_type": "text"
      },
      "source": [
        "#### Example 3: Word2Vec Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuy2TaDI7Baz",
        "colab_type": "text"
      },
      "source": [
        "Redefine some example-specific arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GmmDhavcc6FU",
        "colab": {}
      },
      "source": [
        "args.save_dir = 'w2v_demo_model'      # model is saved in saved_models/glove_demo_model under job_dir\n",
        "args.restore_dir = 'w2v_demo_model'   # model is restored from saved_models/ glove_demo_model under job_dir\n",
        "args.log_dir = 'w2v_demo_model'       # logs are written to logs/glove_demo_model under job_dir\n",
        "\n",
        "args.mode = 'word2vec'\n",
        "args.neg_samples = 16 # no negative samples"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IGS7J2Ubc6FY"
      },
      "source": [
        "Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5x83__eGc6FZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3c33715-76dd-480c-b47c-43c6beb0c10c"
      },
      "source": [
        "train_file_name = normalized_train_file_name(args)\n",
        "train_file_path = os.path.join(args.job_dir, 'model_data', train_file_name)\n",
        "\n",
        "word2id, id2word, word_counts, id_counts, skips_paths = load_process_data(train_file_name, args, remove_zero = False)\n",
        "vocabulary_size = max(word2id.values()) + 1\n",
        "\n",
        "arr_counts = np.array([id_counts[i] for i in range(len(id2word))], dtype = np.float32)\n",
        "arr_counts[:] = arr_counts**args.po\n",
        "unigram = arr_counts/arr_counts.sum()\n",
        "\n",
        "dataset = create_dataset_from_stored_batches(skips_paths, args.stored_batch_size, batch_size = args.batch_size, sampling_distribution = unigram, threshold = args.threshold, po = args.po, neg_samples = args.neg_samples)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Key and value files for stored_enwik8_maxsize_50000_minocc_10_window_10_storedbatch_131072 already exist. Nothing to be done. Consider checking contents.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4AAoxxnZc6Fb"
      },
      "source": [
        "Create and compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3EIL4OdLc6Fc",
        "colab": {}
      },
      "source": [
        "train_model = Word2VecModel(vocabulary_size, args.embedding_size, args.neg_samples, learning_rate=2e-3, word2id = word2id, id2word = id2word)\n",
        "train_model.compile(loss = train_model.loss, optimizer = train_model.optimizer)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "In6PwyoFc6Fd"
      },
      "source": [
        "Restore model weights if available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mk5IFsNyc6Fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "03d2cf6f-448c-4f80-ed57-6bdf5e83f986"
      },
      "source": [
        "epochs_trained_ = 0\n",
        "try:\n",
        "    if args.restore_dir:\n",
        "        _, epochs_trained_ = train_model.load_model(os.path.join(args.job_dir, 'saved_models', args.restore_dir))    \n",
        "except FileNotFoundError: # if no file to restore from found\n",
        "    print('Model weights could not be found.')\n",
        "print(f'Epochs trained: {epochs_trained_}')  "
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model weights could not be found.\n",
            "Epochs trained: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fr4meTiDc6Ff"
      },
      "source": [
        "Define callbacks for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9DzPetOnc6Ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "42874219-d2d6-4392-a5d9-ac1a7890a65e"
      },
      "source": [
        "similarity_tests_dict = get_similarity_tests(args.job_dir)\n",
        "similarity_callbacks = train_model.get_similarity_tests_callbacks(similarity_tests_dict, ['target', 'context', 'added', 'concat'], ['l2', 'cos'], args.job_dir, args.log_dir)\n",
        "\n",
        "analogy_tests_dict = get_analogy_tests(args.job_dir)\n",
        "analogy_callbacks = train_model.get_analogy_tests_callbacks(analogy_tests_dict, ['target', 'context'], ['l2', 'cos'], args.job_dir, args.log_dir, group_dict= ANALOGY_TEST_GROUPS)\n",
        "\n",
        "save_callbacks = train_model.get_save_callbacks(os.path.join(args.job_dir, 'saved_models' , args.save_dir) if args.save_dir else None, args, period = 1)\n",
        "loss_callback = train_model.get_loss_callback(args.job_dir, args.log_dir)\n",
        "lr_callback = LearningRateScheduler(lr_scheduler_factory(args.learning_rate))\n",
        "\n",
        "callbacks = save_callbacks + [loss_callback] + similarity_callbacks + analogy_callbacks + [lr_callback]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9SIOmxOOc6Fh"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SOi4Zuotc6Fh",
        "colab": {}
      },
      "source": [
        "train_model.fit(dataset, epochs = args.num_epochs, callbacks = callbacks, initial_epoch=epochs_trained_)# 182ms/step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW5c_63-a497",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L91V6bloUfTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "# %reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IylltIPjUNqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir $JOB_DIR/logs/ --port=9009"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}